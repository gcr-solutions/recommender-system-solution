{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "import tarfile\n",
    "import glob\n",
    "from tensorflow.contrib import predictor\n",
    "\n",
    "# tqdm.pandas()\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "# bucket = os.environ.get(\"BUCKET_NAME\", \" \")\n",
    "# raw_data_folder = os.environ.get(\"RAW_DATA\", \" \")\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)\n",
    "# tqdm_notebook().pandas()\n",
    "s3client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887\n",
      "prefix='sample-data'\n",
      "file preparation: download src key sample-data/feature/recommend-list/news/recall_batch_result.pickle to dst key info/recall_batch_result.pickle\n",
      "file preparation: download src key sample-data/feature/recommend-list/portrait/portrait.pickle to dst key info/portrait.pickle\n",
      "file preparation: download src key sample-data/feature/content/inverted-list/news_id_news_property_dict.pickle to dst key info/news_id_news_property_dict.pickle\n",
      "file preparation: download src key sample-data/model/rank/action/dkn/latest/model.tar.gz to dst key info/model.tar.gz\n",
      "file preparation: download src key sample-data/model/rank/content/dkn_embedding_latest/dkn_entity_embedding.npy to dst key info/dkn_entity_embedding.npy\n",
      "file preparation: download src key sample-data/model/rank/content/dkn_embedding_latest/dkn_context_embedding.npy to dst key info/dkn_context_embedding.npy\n",
      "file preparation: download src key sample-data/model/rank/content/dkn_embedding_latest/dkn_word_embedding.npy to dst key info/dkn_word_embedding.npy\n",
      "length of news_id v.s. news_property 2660\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 从s3同步数据\n",
    "########################################\n",
    "\n",
    "\n",
    "def sync_s3(file_name_list, s3_folder, local_folder):\n",
    "    for f in file_name_list:\n",
    "        print(\"file preparation: download src key {} to dst key {}\".format(os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f)))\n",
    "        s3client.download_file(bucket, os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f))\n",
    "\n",
    "\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    print(\"upload s3://{}/{}\".format(bucket, key))\n",
    "    with open(filename, 'rb') as f:  # Read in binary mode\n",
    "        # return s3client.upload_fileobj(f, bucket, key)\n",
    "        return s3client.put_object(\n",
    "            ACL='bucket-owner-full-control',\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=f\n",
    "        )\n",
    "\n",
    "def write_str_to_s3(content, bucket, key):\n",
    "    print(\"write s3://{}/{}, content={}\".format(bucket, key, content))\n",
    "    s3client.put_object(Body=str(content).encode(\"utf8\"), Bucket=bucket, Key=key, ACL='bucket-owner-full-control')\n",
    "\n",
    "default_bucket = 'aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887'\n",
    "default_prefix = 'sample-data'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bucket', type=str, default=default_bucket)\n",
    "parser.add_argument('--prefix', type=str, default=default_prefix)\n",
    "args, _ = parser.parse_known_args()\n",
    "bucket = args.bucket\n",
    "prefix = args.prefix\n",
    "\n",
    "print(\"bucket={}\".format(bucket))\n",
    "print(\"prefix='{}'\".format(prefix))\n",
    "\n",
    "out_s3_path = \"s3://{}/{}/feature/content/inverted-list\".format(bucket, prefix)\n",
    "\n",
    "local_folder = 'info'\n",
    "if not os.path.exists(local_folder):\n",
    "    os.makedirs(local_folder)\n",
    "# recall batch 结果记载\n",
    "file_name_list = ['recall_batch_result.pickle']\n",
    "s3_folder = '{}/feature/recommend-list/news'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "# 用户画像数据加载\n",
    "file_name_list = ['portrait.pickle']\n",
    "s3_folder = '{}/feature/recommend-list/portrait'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "# 倒排列表的pickle文件\n",
    "file_name_list = ['news_id_news_property_dict.pickle']\n",
    "s3_folder = '{}/feature/content/inverted-list/'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "# dkn模型文件下载\n",
    "file_name_list = ['model.tar.gz']\n",
    "s3_folder = '{}/model/rank/action/dkn/latest/'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "file_name_list = ['dkn_entity_embedding.npy','dkn_context_embedding.npy','dkn_word_embedding.npy']\n",
    "s3_folder = '{}/model/rank/content/dkn_embedding_latest/'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "\n",
    "# 加载pickle文件\n",
    "file_to_load = open(\"info/recall_batch_result.pickle\", \"rb\")\n",
    "recall_batch_result = pickle.load(file_to_load)\n",
    "file_to_load = open(\"info/portrait.pickle\", \"rb\")\n",
    "user_portrait = pickle.load(file_to_load)\n",
    "file_to_load = open(\"info/news_id_news_property_dict.pickle\", \"rb\")\n",
    "dict_id_property_pddf = pickle.load(file_to_load)\n",
    "print(\"length of news_id v.s. news_property {}\".format(len(dict_id_property_pddf)))\n",
    "# 解压缩dkn模型\n",
    "tar = tarfile.open(\"info/model.tar.gz\", \"r\")\n",
    "file_names = tar.getnames()\n",
    "for file_name in file_names:\n",
    "    tar.extract(file_name, \"info/\")\n",
    "tar.close\n",
    "model_extract_dir = \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rank():\n",
    "\n",
    "    def __init__(self, user_portrait, news_id_news_property):\n",
    "        model_extract_dir = 'info'\n",
    "        self.entity_embed = np.load(\"info/dkn_entity_embedding.npy\")\n",
    "        self.context_embed = np.load(\"info/dkn_context_embedding.npy\")\n",
    "        self.word_embed = np.load(\"info/dkn_word_embedding.npy\")\n",
    "        self.user_portrait = user_portrait\n",
    "        self.news_id_news_property = news_id_news_property\n",
    "        for name in glob.glob(os.path.join(model_extract_dir, '**', 'saved_model.pb'), recursive=True):\n",
    "            logging.info(\"found model saved_model.pb in {} !\".format(name))\n",
    "            model_path = '/'.join(name.split('/')[0:-1])\n",
    "        self.model = predictor.from_saved_model(model_path)\n",
    "        self.fill_array = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    def RankProcess(self, request, context):\n",
    "        logging.info('rank_process start')\n",
    "\n",
    "        # Retrieve request data        \n",
    "        reqDicts = Any()\n",
    "        request.dicts.Unpack(reqDicts)\n",
    "        reqData = json.loads(reqDicts.value, encoding='utf-8')\n",
    "        user_id = reqData['user_id']\n",
    "        recall_result = reqData['recall_result']\n",
    "        logging.info('user_id -> {}'.format(user_id))\n",
    "        logging.info('recall_result -> {}'.format(recall_result))\n",
    "\n",
    "        #TODO need to call customer service to get real data\n",
    "        user_clicks_set = ['6553003847780925965','6553082318746026500','6522187689410691591']\n",
    "        # user_clicks_set_redis = rCache.get_data_from_hash(user_id_click_dict, user_id)\n",
    "        # if bool(user_clicks_set_redis):\n",
    "        #     logging.info('user_clicks_set_redis {}'.format(user_clicks_set_redis))\n",
    "        #     user_clicks_set = json.loads(user_clicks_set_redis, encoding='utf-8')\n",
    "\n",
    "        rank_result = self.generate_rank_result(recall_result, self.news_id_entity_ids_dict, self.news_id_word_ids_dict, user_clicks_set)\n",
    "\n",
    "        logging.info(\"rank result {}\".format(rank_result))\n",
    "\n",
    "        rankProcessResponseAny = Any()\n",
    "        rankProcessResponseAny.value =  json.dumps(rank_result).encode('utf-8')\n",
    "        rankProcessResponse = service_pb2.RankProcessResponse(code=0, description='rank process with success')\n",
    "        rankProcessResponse.results.Pack(rankProcessResponseAny)        \n",
    "\n",
    "        logging.info(\"rank process complete\") \n",
    "        return rankProcessResponse\n",
    "\n",
    "    def generate_rank_result(self, recall_result_pddf):\n",
    "        logging.info('generate_rank_result start')\n",
    "        news_words_index = []\n",
    "        news_entity_index = []\n",
    "        click_words_index = []\n",
    "        click_entity_index = []\n",
    "        # debug for mingtong\n",
    "        temp_user_clicks_set = []\n",
    "        for i in range(8):\n",
    "            temp_user_clicks_set.append('6552147830184608263')\n",
    "        \n",
    "        recall_result = recall_result_pddf['news_id'].split('[')[1].split(']')[0].split(',')\n",
    "        user_id = recall_result_pddf['user_id']\n",
    "        user_clicks_set = self.user_portrait[str(user_id)]['click_sets']\n",
    "        \n",
    "        filter_recall_result = []\n",
    "        for recall_item_raw in recall_result:\n",
    "            recall_item = recall_item_raw.split(\"'\")[1]\n",
    "            recall_item = recall_item.split(\"'\")[0]\n",
    "            filter_recall_result.append(recall_item)\n",
    "            logging.info('recall_item news id {}'.format(recall_item))\n",
    "            logging.info('news_id_word_ids_dict {}'.format(self.news_id_news_property[str(recall_item)]['words']))\n",
    "            logging.info('news_id_entity_ids_dict {}'.format(self.news_id_news_property[str(recall_item)]['entities']))\n",
    "                \n",
    "            news_words_index.append(self.news_id_news_property[str(recall_item)]['words'])\n",
    "            news_entity_index.append(self.news_id_news_property[str(recall_item)]['entities'])\n",
    "\n",
    "            click_length = len(user_clicks_set)\n",
    "            count = 0\n",
    "            while click_length > 0 and count < 8:\n",
    "                click_index = user_clicks_set[click_length - 1]\n",
    "                logging.info('clicked_item_id {}'.format(click_index))\n",
    "                logging.info('news_id_word_ids_dict {}'.format(self.news_id_news_property[str(click_index)]['words']))\n",
    "                logging.info('news_id_entity_ids_dict {}'.format(self.news_id_news_property[str(click_index)]['entities']))\n",
    "                click_words_index.append(self.news_id_news_property[str(click_index)]['words'])\n",
    "                click_entity_index.append(self.news_id_news_property[str(click_index)]['entities'])\n",
    "                click_length = click_length -1\n",
    "                count = count + 1\n",
    "\n",
    "            while count < 8:\n",
    "                logging.info('add 0 because user_clicks_set length is less than 8')\n",
    "                click_words_index.append(self.fill_array)\n",
    "                click_entity_index.append(self.fill_array)\n",
    "                count = count + 1\n",
    "            # for clicked_item_id in temp_user_clicks_set:\n",
    "            #     logging.info('clicked_item_id {}'.format(clicked_item_id))\n",
    "            #     logging.info('news_id_word_ids_dict {}'.format(news_id_word_ids_dict[clicked_item_id]))\n",
    "            #     logging.info('news_id_entity_ids_dict {}'.format(news_id_entity_ids_dict[clicked_item_id]))\n",
    "            #     click_words_index.append(news_id_word_ids_dict[clicked_item_id])\n",
    "            #     click_entity_index.append(news_id_entity_ids_dict[clicked_item_id])\n",
    "\n",
    "        for idx in news_words_index:\n",
    "            logging.info(\"news words len {} with array {}\".format(len(idx), idx))\n",
    "        for idx in news_entity_index:\n",
    "            logging.info(\"news entities len {} with array {}\".format(len(idx), idx))\n",
    "        for idx in click_entity_index:\n",
    "            logging.info(\"click entity len {} with array {}\".format(len(idx), idx))\n",
    "        for idx in click_words_index:\n",
    "            logging.info(\"click word len {} with array {}\".format(len(idx), idx))          \n",
    "\n",
    "        news_words_index_np = np.array(news_words_index)\n",
    "        news_entity_index_np = np.array(news_entity_index)\n",
    "        click_words_index_np = np.array(click_words_index)\n",
    "        click_entity_index_np = np.array(click_entity_index)        \n",
    "\n",
    "        logging.info('start create input_dict')\n",
    "        input_dict = {}\n",
    "        input_dict['click_entities'] = self.entity_embed[click_entity_index_np]\n",
    "        input_dict['click_words'] = self.word_embed[click_words_index_np]\n",
    "        input_dict['news_entities'] = self.entity_embed[news_entity_index_np]\n",
    "        input_dict['news_words'] = self.word_embed[news_words_index_np]\n",
    "        logging.info(\"check input shape!\")\n",
    "        logging.info(\"input click entities shape {}\".format(input_dict['click_entities'].shape))\n",
    "        logging.info(\"input click words shape {}\".format(input_dict['click_words'].shape))\n",
    "        logging.info(\"input news entities shape {}\".format(input_dict['news_entities'].shape))\n",
    "        logging.info(\"input news words shape {}\".format(input_dict['news_words'].shape))\n",
    "\n",
    "        output = self.model(input_dict)\n",
    "\n",
    "        logging.info('output {} from model'.format(output))\n",
    "\n",
    "        output_prob = output['prob']\n",
    "        rank_result = []\n",
    "        i = 0\n",
    "        while i < len(output_prob):\n",
    "            rank_result.append({\n",
    "                filter_recall_result[i]: str(output_prob[i])\n",
    "            })\n",
    "            i = i + 1\n",
    "\n",
    "        return rank_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from info/1609592347/variables/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from info/1609592347/variables/variables\n"
     ]
    }
   ],
   "source": [
    "batch_rank = Rank(user_portrait, dict_id_property_pddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>news_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52a23654-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>['6552465493255520771', '6552333627890336263',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52a238fc-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>['6552465493255520771', '6552333627890336263',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52a239c4-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>['6552465493255520771', '6552333627890336263',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52a23a5a-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>['6552465493255520771', '6552333627890336263',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52a23adc-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>['6552465493255520771', '6552333627890336263',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id  \\\n",
       "0  52a23654-9dc3-11eb-a364-acde48001122   \n",
       "1  52a238fc-9dc3-11eb-a364-acde48001122   \n",
       "2  52a239c4-9dc3-11eb-a364-acde48001122   \n",
       "3  52a23a5a-9dc3-11eb-a364-acde48001122   \n",
       "4  52a23adc-9dc3-11eb-a364-acde48001122   \n",
       "\n",
       "                                             news_id  \n",
       "0  ['6552465493255520771', '6552333627890336263',...  \n",
       "1  ['6552465493255520771', '6552333627890336263',...  \n",
       "2  ['6552465493255520771', '6552333627890336263',...  \n",
       "3  ['6552465493255520771', '6552333627890336263',...  \n",
       "4  ['6552465493255520771', '6552333627890336263',...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 整理recall结果\n",
    "data_input_pddf_dict = {}\n",
    "data_input_pddf_dict['user_id'] = []\n",
    "data_input_pddf_dict['news_id'] = []\n",
    "for user_k, result_v in recall_batch_result.items():\n",
    "        data_input_pddf_dict['user_id'].append(str(user_k))\n",
    "        data_input_pddf_dict['news_id'].append(str(list(result_v.keys())))\n",
    "data_input_pddf = pd.DataFrame.from_dict(data_input_pddf_dict)\n",
    "\n",
    "data_input_pddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>news_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>52a2b624-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>['6552465493255520771', '6552333627890336263',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_id  \\\n",
       "267  52a2b624-9dc3-11eb-a364-acde48001122   \n",
       "\n",
       "                                               news_id  \n",
       "267  ['6552465493255520771', '6552333627890336263',...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input_pddf_test = data_input_pddf.sample(1)\n",
    "data_input_pddf_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_pddf['rank_score'] = data_input_pddf.apply(batch_rank.generate_rank_result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:00<00:00, 4217.26it/s]\n"
     ]
    }
   ],
   "source": [
    "rank_result = {}\n",
    "for reviewerID, hist in tqdm(data_input_pddf.groupby('user_id')):\n",
    "    score_list = hist['rank_score'].tolist()[0]\n",
    "    id_score_dict = dict(pair for d in score_list for pair in d.items())\n",
    "    sort_id_score_dict = {k: v for k, v in sorted(id_score_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    rank_result[reviewerID] = sort_id_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/feature/recommend-list/news/rank_batch_result.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'K8815E305YFSSBE8',\n",
       "  'HostId': 'dmLXuaiUKSSHUDBLmZ1lD+ho+twFNBX6bu4wRAtYJ5tbbSy08prmsDjsZmAAyK/2y13OCKyjzrA=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'dmLXuaiUKSSHUDBLmZ1lD+ho+twFNBX6bu4wRAtYJ5tbbSy08prmsDjsZmAAyK/2y13OCKyjzrA=',\n",
       "   'x-amz-request-id': 'K8815E305YFSSBE8',\n",
       "   'date': 'Tue, 20 Apr 2021 01:57:57 GMT',\n",
       "   'etag': '\"ad60d41a12d7ed32d1debbf9bcafe4e1\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"ad60d41a12d7ed32d1debbf9bcafe4e1\"'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'info/rank_batch_result.pickle'\n",
    "output_file = open(file_name, 'wb')\n",
    "pickle.dump(rank_result, output_file)\n",
    "output_file.close()\n",
    "\n",
    "write_to_s3(file_name,\n",
    "            bucket,\n",
    "            '{}/feature/recommend-list/news/rank_batch_result.pickle'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_context_embedding.npy to s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/dkn_context_embedding.npy\n",
      "copy: s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_entity_embedding.npy to s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/dkn_entity_embedding.npy\n",
      "copy: s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_word_embedding.npy to s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/dkn_word_embedding.npy\n",
      "copy: s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/action/dkn/latest/model.tar.gz to s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/model/rank/action/dkn/latest/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 cp s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_context_embedding.npy s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/dkn_context_embedding.npy --acl bucket-owner-full-control\n",
    "# !aws s3 cp s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_entity_embedding.npy s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/dkn_entity_embedding.npy --acl bucket-owner-full-control\n",
    "# !aws s3 cp s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_word_embedding.npy s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/dkn_word_embedding.npy --acl bucket-owner-full-control\n",
    "# !aws s3 cp s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/action/dkn/latest/model.tar.gz s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/model/rank/action/dkn/latest/model.tar.gz --acl bucket-owner-full-control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 6552400705523548680 v 0.05671838\n",
      "k 6552341601450983944 v 0.05580449\n",
      "k 6552015749588713997 v 0.052325606\n",
      "k 6552459085898318343 v 0.05156085\n",
      "k 6551717707065065987 v 0.05155331\n",
      "k 6552288949518205448 v 0.051062495\n",
      "k 6552426608496476685 v 0.049799293\n",
      "k 6552386898592006670 v 0.049290717\n",
      "k 6552496722042421773 v 0.04928559\n",
      "k 6525341647792767502 v 0.04853931\n",
      "k 6552207083696030216 v 0.048426867\n",
      "k 6552428222233969165 v 0.048256338\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for k, v in sort_id_score_dict.items():\n",
    "    print(\"k {} v {}\".format(k,v))\n",
    "    if n > 10:\n",
    "        break\n",
    "    n = n + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
