{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portrait batch logic\n",
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887\n",
      "prefix='sample-data'\n",
      "file preparation: download src key sample-data/system/action-data/action.csv to dst key info/action.csv\n",
      "file preparation: download src key sample-data/system/item-data/item.csv to dst key info/item.csv\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 从s3同步数据\n",
    "########################################\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def sync_s3(file_name_list, s3_folder, local_folder):\n",
    "    for f in file_name_list:\n",
    "        print(\"file preparation: download src key {} to dst key {}\".format(os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f)))\n",
    "        s3client.download_file(bucket, os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f))\n",
    "\n",
    "\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    print(\"upload s3://{}/{}\".format(bucket, key))\n",
    "    with open(filename, 'rb') as f:  # Read in binary mode\n",
    "        # return s3client.upload_fileobj(f, bucket, key)\n",
    "        return s3client.put_object(\n",
    "            ACL='bucket-owner-full-control',\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=f\n",
    "        )\n",
    "\n",
    "def write_str_to_s3(content, bucket, key):\n",
    "    print(\"write s3://{}/{}, content={}\".format(bucket, key, content))\n",
    "    s3client.put_object(Body=str(content).encode(\"utf8\"), Bucket=bucket, Key=key, ACL='bucket-owner-full-control')\n",
    "\n",
    "default_bucket = 'aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887'\n",
    "default_prefix = 'sample-data'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bucket', type=str, default=default_bucket)\n",
    "parser.add_argument('--prefix', type=str, default=default_prefix)\n",
    "args, _ = parser.parse_known_args()\n",
    "bucket = args.bucket\n",
    "prefix = args.prefix\n",
    "\n",
    "print(\"bucket={}\".format(bucket))\n",
    "print(\"prefix='{}'\".format(prefix))\n",
    "\n",
    "out_s3_path = \"s3://{}/{}/feature/content/inverted-list\".format(bucket, prefix)\n",
    "\n",
    "local_folder = 'info'\n",
    "if not os.path.exists(local_folder):\n",
    "    os.makedirs(local_folder)\n",
    "# 行为/物品数据同步\n",
    "file_name_list = ['action.csv']\n",
    "s3_folder = '{}/system/action-data'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "file_name_list = ['recall_config.pickle']\n",
    "s3_folder = '{}/model/recall'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_filter_action = pd.read_csv('info/action.csv',sep='_!_',names=['user_id','news_id','timestamp','action_type','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>action_type</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52a23654-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6552345461607367172</td>\n",
       "      <td>1618477588</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52a23654-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6552332581256299016</td>\n",
       "      <td>1618472565</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52a23654-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6552130363123040771</td>\n",
       "      <td>1618467016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52a23654-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6475484594673025293</td>\n",
       "      <td>1618462187</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52a238fc-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6552277802022863374</td>\n",
       "      <td>1618468013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id              news_id   timestamp  \\\n",
       "0  52a23654-9dc3-11eb-a364-acde48001122  6552345461607367172  1618477588   \n",
       "1  52a23654-9dc3-11eb-a364-acde48001122  6552332581256299016  1618472565   \n",
       "2  52a23654-9dc3-11eb-a364-acde48001122  6552130363123040771  1618467016   \n",
       "3  52a23654-9dc3-11eb-a364-acde48001122  6475484594673025293  1618462187   \n",
       "4  52a238fc-9dc3-11eb-a364-acde48001122  6552277802022863374  1618468013   \n",
       "\n",
       "   action_type  action  \n",
       "0            1       0  \n",
       "1            1       0  \n",
       "2            1       0  \n",
       "3            1       1  \n",
       "4            1       0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter_action.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>action_type</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6320151429650579970</td>\n",
       "      <td>1</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6418014704991469826</td>\n",
       "      <td>1</td>\n",
       "      <td>1.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6422082903882072321</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6422169653342109954</td>\n",
       "      <td>1</td>\n",
       "      <td>2.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6423135627981619458</td>\n",
       "      <td>1</td>\n",
       "      <td>2.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               news_id  action_type    action\n",
       "1  6320151429650579970            1  3.333333\n",
       "2  6418014704991469826            1  1.111111\n",
       "3  6422082903882072321            1  0.000000\n",
       "4  6422169653342109954            1  2.222222\n",
       "5  6423135627981619458            1  2.222222"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "df_item_stats = df_filter_action[['news_id','action_type','action']]\n",
    "df_item_stats = df_item_stats.groupby(['news_id','action_type']).sum()\n",
    "df_item_stats = df_item_stats.reset_index()\n",
    "df_item_stats['action'] = df_item_stats['action'] / df_item_stats['action'].abs().max() * 10.0\n",
    "df_item_stats = df_item_stats.drop([0])\n",
    "df_item_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_merge_result = pd.merge(df_filter_item, df_item_stats, on=\"news_id\", how=\"left\").drop(columns=['action_type'])\n",
    "pd_merge_result = pd_merge_result.fillna(0)\n",
    "row_has_NaN = pd_merge_result.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>type_code</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>new</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6552418723179790856</td>\n",
       "      <td>102</td>\n",
       "      <td>news_entertainment</td>\n",
       "      <td>谢娜三喜临门何炅送祝福吴昕送祝福只有沈梦辰不一样</td>\n",
       "      <td>杜海涛,谢娜,何炅,沈梦辰,吴昕,快本</td>\n",
       "      <td>0</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6552390851157295629</td>\n",
       "      <td>102</td>\n",
       "      <td>news_entertainment</td>\n",
       "      <td>杨幂景甜徐冬冬唐嫣不好好穿衣却美的有趣又撩人</td>\n",
       "      <td>杨幂,徐冬冬,背带裙,大唐荣耀,唐嫣,景甜</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6552309039697494532</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>亚洲杯夺冠赔率日本伊朗领衔中国竟与泰国并列</td>\n",
       "      <td>土库曼斯坦,乌兹别克斯坦,亚洲杯,赔率,小组赛</td>\n",
       "      <td>0</td>\n",
       "      <td>2.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6552452056043487748</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>马夏尔要去切尔西可以商量不过穆里尼奥的要价是4000万加威廉</td>\n",
       "      <td>威廉,曼联,穆里尼奥,布莱顿,马夏尔</td>\n",
       "      <td>0</td>\n",
       "      <td>4.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6552466727995703815</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>昔日中超金靴半场独造6球虐爆辽足华夏送走他后悔吗</td>\n",
       "      <td>阿洛,阿洛伊西奥,华夏幸福,埃尔纳内斯,穆里奇</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>6521968288589677060</td>\n",
       "      <td>100</td>\n",
       "      <td>news_story</td>\n",
       "      <td>李自成为何只在北京当了42天的皇帝看看他在北京42天都干了啥</td>\n",
       "      <td>崇祯皇帝,李自成,山海关,吴三桂,陈圆圆</td>\n",
       "      <td>0</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>6549073874741363213</td>\n",
       "      <td>100</td>\n",
       "      <td>news_story</td>\n",
       "      <td>NT早唐中唐咋回事</td>\n",
       "      <td>顶臀径,唐氏筛查,AFP,孕妈,染色体</td>\n",
       "      <td>0</td>\n",
       "      <td>4.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>6546844349055894023</td>\n",
       "      <td>100</td>\n",
       "      <td>news_story</td>\n",
       "      <td>兴唐119罗成是天下第七魏文通是天下第九罗成几招能获胜</td>\n",
       "      <td>罗士信,魏文通,罗成,秦琼,丁彦平</td>\n",
       "      <td>0</td>\n",
       "      <td>5.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>6555209635756769800</td>\n",
       "      <td>114</td>\n",
       "      <td>stock</td>\n",
       "      <td>富士康拟发行约197亿股股票简称工业富联</td>\n",
       "      <td>股票,中金公司,招股说明书,首次公开发行,富士康</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>6547166344901558798</td>\n",
       "      <td>100</td>\n",
       "      <td>news_story</td>\n",
       "      <td>兴唐121罗成恩将仇报扎伤义父丁彦平丁彦平怎么跟他算账</td>\n",
       "      <td>丁彦平,罗成,报国寺,瓦岗山,南陈</td>\n",
       "      <td>0</td>\n",
       "      <td>2.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2660 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  news_id  type_code                type  \\\n",
       "0     6552418723179790856        102  news_entertainment   \n",
       "1     6552390851157295629        102  news_entertainment   \n",
       "2     6552309039697494532        103         news_sports   \n",
       "3     6552452056043487748        103         news_sports   \n",
       "4     6552466727995703815        103         news_sports   \n",
       "...                   ...        ...                 ...   \n",
       "2655  6521968288589677060        100          news_story   \n",
       "2656  6549073874741363213        100          news_story   \n",
       "2657  6546844349055894023        100          news_story   \n",
       "2658  6555209635756769800        114               stock   \n",
       "2659  6547166344901558798        100          news_story   \n",
       "\n",
       "                               title                  keywords  new  \\\n",
       "0           谢娜三喜临门何炅送祝福吴昕送祝福只有沈梦辰不一样       杜海涛,谢娜,何炅,沈梦辰,吴昕,快本    0   \n",
       "1             杨幂景甜徐冬冬唐嫣不好好穿衣却美的有趣又撩人     杨幂,徐冬冬,背带裙,大唐荣耀,唐嫣,景甜    0   \n",
       "2              亚洲杯夺冠赔率日本伊朗领衔中国竟与泰国并列   土库曼斯坦,乌兹别克斯坦,亚洲杯,赔率,小组赛    0   \n",
       "3     马夏尔要去切尔西可以商量不过穆里尼奥的要价是4000万加威廉        威廉,曼联,穆里尼奥,布莱顿,马夏尔    0   \n",
       "4           昔日中超金靴半场独造6球虐爆辽足华夏送走他后悔吗   阿洛,阿洛伊西奥,华夏幸福,埃尔纳内斯,穆里奇    0   \n",
       "...                              ...                       ...  ...   \n",
       "2655  李自成为何只在北京当了42天的皇帝看看他在北京42天都干了啥      崇祯皇帝,李自成,山海关,吴三桂,陈圆圆    0   \n",
       "2656                       NT早唐中唐咋回事       顶臀径,唐氏筛查,AFP,孕妈,染色体    0   \n",
       "2657     兴唐119罗成是天下第七魏文通是天下第九罗成几招能获胜         罗士信,魏文通,罗成,秦琼,丁彦平    0   \n",
       "2658            富士康拟发行约197亿股股票简称工业富联  股票,中金公司,招股说明书,首次公开发行,富士康    0   \n",
       "2659     兴唐121罗成恩将仇报扎伤义父丁彦平丁彦平怎么跟他算账         丁彦平,罗成,报国寺,瓦岗山,南陈    0   \n",
       "\n",
       "      popularity  \n",
       "0       3.333333  \n",
       "1       1.111111  \n",
       "2       2.222222  \n",
       "3       4.444444  \n",
       "4       1.111111  \n",
       "...          ...  \n",
       "2655    6.666667  \n",
       "2656    4.444444  \n",
       "2657    5.555556  \n",
       "2658    1.111111  \n",
       "2659    2.222222  \n",
       "\n",
       "[2660 rows x 7 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_merge_result.drop(columns=['popularity']).rename(columns={\"action\":\"popularity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary file /home/ec2-user/.fastNLP/fasthan/fasthan_base/vocab.txt\n",
      "Load pre-trained BERT parameters from file /home/ec2-user/.fastNLP/fasthan/fasthan_base/model.bin.\n"
     ]
    }
   ],
   "source": [
    "# prepare model for batch process\n",
    "os.environ['GRAPH_BUCKET'] = 'sagemaker-us-east-1-002224604296'\n",
    "os.environ['KG_DBPEDIA_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/kg_dbpedia.txt'\n",
    "os.environ['KG_ENTITY_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/entities_dbpedia.dict'\n",
    "os.environ['KG_RELATION_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/relations_dbpedia.dict'\n",
    "os.environ['KG_ENTITY_INDUSTRY_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/entity_industry.txt'\n",
    "os.environ['KG_VOCAB_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/vocab.json'\n",
    "os.environ['DATA_INPUT_KEY'] = ''\n",
    "os.environ['TRAIN_OUTPUT_KEY'] = 'recommender-system-data/model/sort/content/kg/news/gw/'\n",
    "kg_path = os.environ['GRAPH_BUCKET']\n",
    "dbpedia_key = os.environ['KG_DBPEDIA_KEY']\n",
    "entity_key = os.environ['KG_ENTITY_KEY']\n",
    "relation_key = os.environ['KG_RELATION_KEY']\n",
    "entity_industry_key = os.environ['KG_ENTITY_INDUSTRY_KEY']\n",
    "vocab_key = os.environ['KG_VOCAB_KEY']\n",
    "data_input_key = os.environ['DATA_INPUT_KEY']\n",
    "train_output_key = os.environ['TRAIN_OUTPUT_KEY']\n",
    "\n",
    "env = {\n",
    "    'GRAPH_BUCKET': kg_path,\n",
    "    'KG_DBPEDIA_KEY': dbpedia_key,\n",
    "    'KG_ENTITY_KEY': entity_key,\n",
    "    'KG_RELATION_KEY': relation_key,\n",
    "    'KG_ENTITY_INDUSTRY_KEY': entity_industry_key,\n",
    "    'KG_VOCAB_KEY': vocab_key,\n",
    "    'DATA_INPUT_KEY': data_input_key,\n",
    "    'TRAIN_OUTPUT_KEY': train_output_key\n",
    "}\n",
    "graph = kg.Kg(env)  # Where we keep the model when it's loaded\n",
    "model = encoding.encoding(graph, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2660"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filter_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dict_id_keywords for tfidf\n",
    "dict_keywords_id = {}\n",
    "for row in df_filter_item.iterrows():\n",
    "    item_row = row[1]\n",
    "    program_id = str(item_row['news_id'])\n",
    "    for kw in item_row['keywords'].split(','):\n",
    "        if kw not in dict_keywords_id.keys():\n",
    "            dict_keywords_id[kw] = [program_id]\n",
    "            continue\n",
    "        current_list = dict_keywords_id[kw]\n",
    "        current_list.append(program_id)\n",
    "        dict_keywords_id[kw].append(program_id)\n",
    "n_keyword_whole = len(dict_keywords_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(category_property):\n",
    "    if not category_property or str(category_property).lower() in ['nan', 'nr', '']:\n",
    "        return [None]\n",
    "    if not category_property:\n",
    "        return [None]\n",
    "    value = [item.strip() for item in category_property.split(',')]\n",
    "    keywords_tfidf = {}\n",
    "    for keyword in value:\n",
    "        current_score = 1 / len(value)*math.log(n_keyword_whole / len(dict_keywords_id[keyword]))\n",
    "        keywords_tfidf[keyword] = current_score\n",
    "    return keywords_tfidf\n",
    "        \n",
    "def get_category(category_property):\n",
    "    if not category_property or str(category_property).lower() in ['nan', 'nr', '']:\n",
    "        return [None]\n",
    "    if not category_property:\n",
    "        return [None]\n",
    "    return [item.strip().lower() for item in category_property.split(',')]\n",
    "            \n",
    "def get_single_item(item):\n",
    "    if not item or str(item).lower().strip() in ['nan', 'nr', '']:\n",
    "        return [None]\n",
    "    return [str(item).lower().strip()]\n",
    "\n",
    "def get_entities(title):\n",
    "    return model[title]\n",
    "\n",
    "def single_dict(raw_dict, feat, item_id):\n",
    "    if feat not in raw_dict.keys():\n",
    "        raw_dict[feat] = [item_id]\n",
    "    else:\n",
    "        current_list = raw_dict[feat]\n",
    "        current_list.append(item_id)\n",
    "        raw_dict[feat] = current_list\n",
    "\n",
    "def list_dict(raw_dict, feat_list, item_id):\n",
    "    for feat in feat_list:\n",
    "        single_dict(raw_dict, feat, item_id)\n",
    "\n",
    "def update_popularity(item_df, action_df):\n",
    "    pd_merge_result = pd.merge(item_df, action_df, on=\"news_id\", how=\"left\").drop(columns=['action_type'])\n",
    "    pd_merge_result = pd_merge_result.fillna(0)\n",
    "    df_update = pd_merge_result.drop(columns=['popularity']).rename(columns={\"action\":\"popularity\"})\n",
    "    return df_update\n",
    "        \n",
    "def sort_by_score(df):\n",
    "    logging.info(\"sort_by_score() enter, df.columns: {}\".format(df.columns))\n",
    "    df['popularity'].fillna(0, inplace=True)\n",
    "\n",
    "    df['popularity_log'] = np.log1p(df['popularity'])\n",
    "    popularity_log_max = df['popularity_log'].max()\n",
    "    popularity_log_min = df['popularity_log'].min()\n",
    "\n",
    "    df['popularity_scaled'] = ((df['popularity_log'] - popularity_log_min) / (\n",
    "            popularity_log_max - popularity_log_min)) * 10\n",
    "\n",
    "    df_sorted = df.sort_values(by='popularity_scaled', ascending=False)\n",
    "    \n",
    "    df_sorted = df_sorted.drop(\n",
    "        ['popularity_log', 'popularity_scaled'], axis=1)\n",
    "\n",
    "    logging.info(\"sort_by_score() return, df.columns: {}\".format(df_sorted.columns))\n",
    "    return df_sorted\n",
    "\n",
    "def get_bucket_key_from_s3_path(s3_path):\n",
    "    m = re.match(r\"s3://(.*?)/(.*)\", s3_path)\n",
    "    return m.group(1), m.group(2)\n",
    "\n",
    "def gen_pickle_files(df, action_df):\n",
    "    df_update = update_popularity(df, action_df)\n",
    "    df_sort = sort_by_score(df_update)\n",
    "    \n",
    "    news_id_news_property_dict = {}\n",
    "    news_type_news_ids_dict = {}\n",
    "    news_keywords_news_ids_dict = {}\n",
    "    news_entities_news_ids_dict = {}\n",
    "    news_words_news_ids_dict = {}\n",
    "    \n",
    "    for row in df_sort.iterrows():\n",
    "        item_row = row[1]\n",
    "        program_id = str(item_row['news_id'])\n",
    "        current_entities = get_entities(item_row['title'])[0]\n",
    "        current_words = get_entities(item_row['title'])[1]\n",
    "        program_dict = {\n",
    "            'title': get_single_item(item_row['title']),\n",
    "            'type': get_single_item(item_row['type']),\n",
    "            'keywords': get_category(item_row['keywords']),\n",
    "            'tfidf': get_tfidf(item_row['keywords']),\n",
    "            'entities': current_entities,\n",
    "            'words': current_words\n",
    "        }\n",
    "        news_id_news_property_dict[program_id] = program_dict\n",
    "        list_dict(news_type_news_ids_dict, program_dict['type'], program_id)\n",
    "        list_dict(news_keywords_news_ids_dict, program_dict['keywords'], program_id)\n",
    "        list_dict(news_entities_news_ids_dict, program_dict['entities'], program_id)\n",
    "        list_dict(news_words_news_ids_dict, program_dict['words'], program_id)\n",
    "\n",
    "    result_dict = {\n",
    "        'news_id_news_property_dict': news_id_news_property_dict,\n",
    "        'news_type_news_ids_dict': news_type_news_ids_dict,\n",
    "        'news_keywords_news_ids_dict': news_keywords_news_ids_dict,\n",
    "        'news_entities_news_ids_dict': news_entities_news_ids_dict,\n",
    "        'news_words_news_ids_dict': news_words_news_ids_dict\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = gen_pickle_files(df_filter_item, df_item_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_id_news_property_dict.pickle\n",
      "upload s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_type_news_ids_dict.pickle\n",
      "upload s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_keywords_news_ids_dict.pickle\n",
      "upload s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_entities_news_ids_dict.pickle\n",
      "upload s3://aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_words_news_ids_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "bucket, out_prefix = get_bucket_key_from_s3_path(out_s3_path)\n",
    "for dict_name, dict_val in rd.items():\n",
    "    file_name = f'{dict_name}.pickle'\n",
    "    # print(\"pickle =>\", file_name)\n",
    "    out_file = open(file_name, 'wb')\n",
    "    pickle.dump(dict_val, out_file)\n",
    "    out_file.close()\n",
    "    # s3_url = S3Uploader.upload(file_name, out_s3_path)\n",
    "    s3_url = write_to_s3(file_name, bucket, f'{out_prefix}/{file_name}')\n",
    "    logging.info(\"write {}\".format(s3_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 6552418723179790856 v {'title': ['谢娜三喜临门何炅送祝福吴昕送祝福只有沈梦辰不一样'], 'type': ['news_entertainment'], 'keywords': ['杜海涛', '谢娜', '何炅', '沈梦辰', '吴昕', '快本'], 'tfidf': {'杜海涛': 1.1915032285682567, '谢娜': 0.8605173146234215, '何炅': 0.9546056150797302, '沈梦辰': 1.3327195386327908, '吴昕': 1.1915032285682567, '快本': 1.1161723746110805}, 'entities': [40191, 0, 46990, 1871, 5802, 162743, 1871, 5802, 315, 390701, 28, 302, 0, 0, 0, 0], 'words': [559632, 0, 613175, 0, 0, 754092, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552390851157295629 v {'title': ['杨幂景甜徐冬冬唐嫣不好好穿衣却美的有趣又撩人'], 'type': ['news_entertainment'], 'keywords': ['杨幂', '徐冬冬', '背带裙', '大唐荣耀', '唐嫣', '景甜'], 'tfidf': {'杨幂': 1.1161723746110805, '徐冬冬': 1.3327195386327908, '背带裙': 1.5158215867441425, '大唐荣耀': 1.5158215867441425, '唐嫣': 1.3327195386327908, '景甜': 1.3327195386327908}, 'entities': [20585, 130577, 193876, 71718, 28, 2798, 20784, 382, 727, 2, 5876, 121, 67692, 0, 0, 0], 'words': [0, 0, 359872, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552309039697494532 v {'title': ['亚洲杯夺冠赔率日本伊朗领衔中国竟与泰国并列'], 'type': ['news_sports'], 'keywords': ['土库曼斯坦', '乌兹别克斯坦', '亚洲杯', '赔率', '小组赛'], 'tfidf': {'土库曼斯坦': 1.8189859040929712, '乌兹别克斯坦': 1.8189859040929712, '亚洲杯': 1.8189859040929712, '赔率': 1.8189859040929712, '小组赛': 1.8189859040929712}, 'entities': [30465, 6489, 20485, 232, 2944, 16751, 21, 4660, 25, 2253, 13292, 0, 0, 0, 0, 0], 'words': [0, 0, 0, 0, 0, 0, 78314, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552452056043487748 v {'title': ['马夏尔要去切尔西可以商量不过穆里尼奥的要价是4000万加威廉'], 'type': ['news_sports'], 'keywords': ['威廉', '曼联', '穆里尼奥', '布莱顿', '马夏尔'], 'tfidf': {'威廉': 1.5992634463593491, '曼联': 1.339406849533297, '穆里尼奥': 1.339406849533297, '布莱顿': 1.4970983216061509, '马夏尔': 1.4970983216061509}, 'entities': [0, 64, 132, 13858, 45, 8449, 28, 230, 25044, 2, 51584, 10, 18465, 0, 0, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552466727995703815 v {'title': ['昔日中超金靴半场独造6球虐爆辽足华夏送走他后悔吗'], 'type': ['news_sports'], 'keywords': ['阿洛', '阿洛伊西奥', '华夏幸福', '埃尔纳内斯', '穆里奇'], 'tfidf': {'阿洛': 1.8189859040929712, '阿洛伊西奥': 1.5992634463593491, '华夏幸福': 1.8189859040929712, '埃尔纳内斯': 1.8189859040929712, '穆里奇': 1.5992634463593491}, 'entities': [7707, 13823, 70038, 708, 1096, 450178, 268, 2732, 0, 52005, 4709, 30724, 26, 8887, 671, 0], 'words': [0, 0, 0, 0, 0, 0, 88742, 0, 0, 0, 294528, 0, 0, 0, 0, 0]}\n",
      "k 6552404265724281357 v {'title': ['拜仁3比1逆转科隆j罗现世界级做饼'], 'type': ['news_sports'], 'keywords': ['j罗', '科隆', '拜仁', '假动作', '托利索'], 'tfidf': {'J罗': 1.8189859040929712, '科隆': 1.8189859040929712, '拜仁': 1.5992634463593491, '假动作': 1.8189859040929712, '托利索': 1.8189859040929712}, 'entities': [19108, 160, 277, 100, 8486, 34539, 0, 1507, 10631, 190, 11871, 0, 0, 0, 0, 0], 'words': [0, 0, 0, 794012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552327554336817678 v {'title': ['成都摇号选房天立世纪华府彭州清水均价5476不是投资处'], 'type': ['news_house'], 'keywords': ['都江堰', '彭州', '楼盘', '天立世纪华府', '㎡三室两厅两卫户型'], 'tfidf': {'都江堰': 1.3795409886257273, '彭州': 1.5992634463593491, '楼盘': 1.1752107391193307, '天立世纪华府': 1.5992634463593491, '㎡三室两厅两卫户型': 1.5992634463593491}, 'entities': [1564, 8527, 394, 3747, 3689, 166899, 989, 60596, 42472, 6796, 3739, 529169, 28, 10, 0, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 524554, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552236298344595982 v {'title': ['青岛这个区牛了要建奥特莱斯银座还通地铁'], 'type': ['news_house'], 'keywords': ['世茂', '银座', '高新区', '奥特莱斯', '青岛高新区', '项目投资协议'], 'tfidf': {'世茂': 1.3327195386327908, '银座': 1.5158215867441425, '高新区': 1.2475819346717922, '奥特莱斯': 1.5158215867441425, '青岛高新区': 1.5158215867441425, '项目投资协议': 1.5158215867441425}, 'entities': [2376, 43, 31, 786, 3678, 6, 64, 1715, 50349, 44926, 77, 1603, 2888, 0, 0, 0], 'words': [768682, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552134625018249735 v {'title': ['重机云集的醉美江南机车节上谁最美胡军李亚鹏告诉你答案'], 'type': ['news_car'], 'keywords': ['李亚鹏', '印第安', 'bobber', '北极星', '胡军', '车友'], 'tfidf': {'李亚鹏': 1.3327195386327908, '印第安': 1.5158215867441425, 'Bobber': 1.5158215867441425, '北极星': 1.5158215867441425, '胡军': 1.5158215867441425, '车友': 1.5158215867441425}, 'entities': [46890, 12642, 2, 0, 5125, 0, 40, 1020, 213, 727, 68081, 38631, 722, 41, 2093, 0], 'words': [0, 0, 0, 0, 227484, 0, 0, 0, 0, 0, 186541, 372574, 0, 0, 0, 0]}\n",
      "k 6552382749393551885 v {'title': ['汉腾汽车首款mpv想做车市黑马先问问宝骏730同意不'], 'type': ['news_car'], 'keywords': ['mpv', '中控台', 'gm8', 'r18', '汉腾旗下'], 'tfidf': {'MPV': 1.4970983216061509, '中控台': 1.4298038742819084, 'GM8': 1.5992634463593491, 'R18': 1.5992634463593491, '汉腾旗下': 1.8189859040929712}, 'entities': [93769, 301, 1184, 1191, 24457, 427, 190, 11956, 11745, 1008, 14235, 53589, 32647, 1493, 28, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552296183421731336 v {'title': ['李嘉诚这次火了投资51亿新车颜值不输特斯拉董明珠都羡慕'], 'type': ['news_car'], 'keywords': ['雷克萨斯', '李嘉诚', '新车', '董明珠', '保险杠', '特斯拉'], 'tfidf': {'雷克萨斯': 1.0644798865604408, '李嘉诚': 1.025081756883069, '新车': 1.1496174905214391, '董明珠': 1.043619362734773, '保险杠': 1.3327195386327908, '特斯拉': 0.9330703264997291}, 'entities': [20703, 43, 157, 3774, 6, 104, 136653, 5827, 13458, 28, 7909, 2659, 17295, 46, 8693, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 160973, 631103, 0, 0, 0]}\n",
      "k 6552387440152150536 v {'title': ['美出台国防授权法案应对中俄竞争国防预算超7000亿美元'], 'type': ['news_military'], 'keywords': ['理查森', '北大西洋', '俄罗斯', '网络战', '司令部'], 'tfidf': {'理查森': 1.230098108259683, '北大西洋': 1.2773758638725292, '俄罗斯': 0.7727641807220538, '网络战': 1.339406849533297, '司令部': 1.1079162917950884}, 'entities': [727, 1965, 4826, 2017, 6478, 828, 33, 32, 5710, 1097, 4826, 3262, 1164, 74592, 148, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for k, v in rd['news_id_news_property_dict'].items():\n",
    "    print(\"k {} v {}\".format(k,v))\n",
    "    if n > 10:\n",
    "        break\n",
    "    n = n + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
