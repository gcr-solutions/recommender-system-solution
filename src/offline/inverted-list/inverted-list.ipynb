{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys path is ['/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python36.zip', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/lib-dynload', '', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/dglke-0.1.0.dev0-py3.6.egg', '/home/ec2-user/workplace/Aws-gcr-sc-recommender-system/src/offline/docker/batch/container/batch/src/fasthan', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/extensions', '/home/ec2-user/.ipython']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import kg\n",
    "import encoding\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "tqdm.pandas()\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "# bucket = os.environ.get(\"BUCKET_NAME\", \" \")\n",
    "# raw_data_folder = os.environ.get(\"RAW_DATA\", \" \")\n",
    "\n",
    "s3client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887\n",
      "prefix='sample-data'\n",
      "file preparation: download src key sample-data/system/item-data/item.csv to dst key info/item.csv\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 从s3同步数据\n",
    "########################################\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def sync_s3(file_name_list, s3_folder, local_folder):\n",
    "    for f in file_name_list:\n",
    "        print(\"file preparation: download src key {} to dst key {}\".format(os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f)))\n",
    "        s3client.download_file(bucket, os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f))\n",
    "\n",
    "\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    print(\"upload s3://{}/{}\".format(bucket, key))\n",
    "    with open(filename, 'rb') as f:  # Read in binary mode\n",
    "        # return s3client.upload_fileobj(f, bucket, key)\n",
    "        return s3client.put_object(\n",
    "            ACL='bucket-owner-full-control',\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=f\n",
    "        )\n",
    "\n",
    "def write_str_to_s3(content, bucket, key):\n",
    "    print(\"write s3://{}/{}, content={}\".format(bucket, key, content))\n",
    "    s3client.put_object(Body=str(content).encode(\"utf8\"), Bucket=bucket, Key=key, ACL='bucket-owner-full-control')\n",
    "\n",
    "default_bucket = 'aws-gcr-rs-sol-workshop-ap-southeast-1-522244679887'\n",
    "default_prefix = 'sample-data'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bucket', type=str, default=default_bucket)\n",
    "parser.add_argument('--prefix', type=str, default=default_prefix)\n",
    "args, _ = parser.parse_known_args()\n",
    "bucket = args.bucket\n",
    "prefix = args.prefix\n",
    "\n",
    "print(\"bucket={}\".format(bucket))\n",
    "print(\"prefix='{}'\".format(prefix))\n",
    "\n",
    "local_folder = 'info'\n",
    "if not os.path.exists(local_folder):\n",
    "    os.makedirs(local_folder)\n",
    "# 行为/物品数据同步\n",
    "file_name_list = ['action.csv']\n",
    "s3_folder = '{}/system/user-data'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "file_name_list = ['item.csv']\n",
    "s3_folder = '{}/system/item-data'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "# 行为/物品数据加载\n",
    "df_filter_action = pd.read_csv('info/action.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_filter_item = pd.read_csv('info/item.csv',sep='_!_',names=['news_id','type_code','type','title','keywords','popularity','new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>type_code</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>popularity</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6552418723179790856</td>\n",
       "      <td>102</td>\n",
       "      <td>news_entertainment</td>\n",
       "      <td>谢娜三喜临门何炅送祝福吴昕送祝福只有沈梦辰不一样</td>\n",
       "      <td>杜海涛,谢娜,何炅,沈梦辰,吴昕,快本</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6552390851157295629</td>\n",
       "      <td>102</td>\n",
       "      <td>news_entertainment</td>\n",
       "      <td>杨幂景甜徐冬冬唐嫣不好好穿衣却美的有趣又撩人</td>\n",
       "      <td>杨幂,徐冬冬,背带裙,大唐荣耀,唐嫣,景甜</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6552309039697494532</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>亚洲杯夺冠赔率日本伊朗领衔中国竟与泰国并列</td>\n",
       "      <td>土库曼斯坦,乌兹别克斯坦,亚洲杯,赔率,小组赛</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6552452056043487748</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>马夏尔要去切尔西可以商量不过穆里尼奥的要价是4000万加威廉</td>\n",
       "      <td>威廉,曼联,穆里尼奥,布莱顿,马夏尔</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6552466727995703815</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>昔日中超金靴半场独造6球虐爆辽足华夏送走他后悔吗</td>\n",
       "      <td>阿洛,阿洛伊西奥,华夏幸福,埃尔纳内斯,穆里奇</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               news_id  type_code                type  \\\n",
       "0  6552418723179790856        102  news_entertainment   \n",
       "1  6552390851157295629        102  news_entertainment   \n",
       "2  6552309039697494532        103         news_sports   \n",
       "3  6552452056043487748        103         news_sports   \n",
       "4  6552466727995703815        103         news_sports   \n",
       "\n",
       "                            title                 keywords  popularity  new  \n",
       "0        谢娜三喜临门何炅送祝福吴昕送祝福只有沈梦辰不一样      杜海涛,谢娜,何炅,沈梦辰,吴昕,快本           3    0  \n",
       "1          杨幂景甜徐冬冬唐嫣不好好穿衣却美的有趣又撩人    杨幂,徐冬冬,背带裙,大唐荣耀,唐嫣,景甜           9    0  \n",
       "2           亚洲杯夺冠赔率日本伊朗领衔中国竟与泰国并列  土库曼斯坦,乌兹别克斯坦,亚洲杯,赔率,小组赛           2    0  \n",
       "3  马夏尔要去切尔西可以商量不过穆里尼奥的要价是4000万加威廉       威廉,曼联,穆里尼奥,布莱顿,马夏尔           2    0  \n",
       "4        昔日中超金靴半场独造6球虐爆辽足华夏送走他后悔吗  阿洛,阿洛伊西奥,华夏幸福,埃尔纳内斯,穆里奇           0    0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter_action = pd.read_csv('info/action.csv',sep='_!_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter_action.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary file /home/ec2-user/.fastNLP/fasthan/fasthan_base/vocab.txt\n",
      "Load pre-trained BERT parameters from file /home/ec2-user/.fastNLP/fasthan/fasthan_base/model.bin.\n"
     ]
    }
   ],
   "source": [
    "# prepare model for batch process\n",
    "os.environ['GRAPH_BUCKET'] = 'sagemaker-us-east-1-002224604296'\n",
    "os.environ['KG_DBPEDIA_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/kg_dbpedia.txt'\n",
    "os.environ['KG_ENTITY_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/entities_dbpedia.dict'\n",
    "os.environ['KG_RELATION_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/relations_dbpedia.dict'\n",
    "os.environ['KG_ENTITY_INDUSTRY_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/entity_industry.txt'\n",
    "os.environ['KG_VOCAB_KEY'] = 'recommender-system-data/model/sort/content/words/mapping/vocab.json'\n",
    "os.environ['DATA_INPUT_KEY'] = ''\n",
    "os.environ['TRAIN_OUTPUT_KEY'] = 'recommender-system-data/model/sort/content/kg/news/gw/'\n",
    "kg_path = os.environ['GRAPH_BUCKET']\n",
    "dbpedia_key = os.environ['KG_DBPEDIA_KEY']\n",
    "entity_key = os.environ['KG_ENTITY_KEY']\n",
    "relation_key = os.environ['KG_RELATION_KEY']\n",
    "entity_industry_key = os.environ['KG_ENTITY_INDUSTRY_KEY']\n",
    "vocab_key = os.environ['KG_VOCAB_KEY']\n",
    "data_input_key = os.environ['DATA_INPUT_KEY']\n",
    "train_output_key = os.environ['TRAIN_OUTPUT_KEY']\n",
    "\n",
    "env = {\n",
    "    'GRAPH_BUCKET': kg_path,\n",
    "    'KG_DBPEDIA_KEY': dbpedia_key,\n",
    "    'KG_ENTITY_KEY': entity_key,\n",
    "    'KG_RELATION_KEY': relation_key,\n",
    "    'KG_ENTITY_INDUSTRY_KEY': entity_industry_key,\n",
    "    'KG_VOCAB_KEY': vocab_key,\n",
    "    'DATA_INPUT_KEY': data_input_key,\n",
    "    'TRAIN_OUTPUT_KEY': train_output_key\n",
    "}\n",
    "graph = kg.Kg(env)  # Where we keep the model when it's loaded\n",
    "model = encoding.encoding(graph, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2660"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filter_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dict_id_keywords for tfidf\n",
    "dict_keywords_id = {}\n",
    "for row in df_filter_item.iterrows():\n",
    "    item_row = row[1]\n",
    "    program_id = str(item_row['news_id'])\n",
    "    for kw in item_row['keywords'].split(','):\n",
    "        if kw not in dict_keywords_id.keys():\n",
    "            dict_keywords_id[kw] = [program_id]\n",
    "            continue\n",
    "        current_list = dict_keywords_id[kw]\n",
    "        current_list.append(program_id)\n",
    "        dict_keywords_id[kw].append(program_id)\n",
    "n_keyword_whole = len(dict_keywords_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(category_property):\n",
    "    if not category_property or str(category_property).lower() in ['nan', 'nr', '']:\n",
    "        return [None]\n",
    "    if not category_property:\n",
    "        return [None]\n",
    "    value = [item.strip() for item in category_property.split(',')]\n",
    "    keywords_tfidf = {}\n",
    "    for keyword in value:\n",
    "        current_score = 1 / len(value)*math.log(n_keyword_whole / len(dict_keywords_id[keyword]))\n",
    "        keywords_tfidf[keyword] = current_score\n",
    "    return keywords_tfidf\n",
    "        \n",
    "def get_category(category_property):\n",
    "    if not category_property or str(category_property).lower() in ['nan', 'nr', '']:\n",
    "        return [None]\n",
    "    if not category_property:\n",
    "        return [None]\n",
    "    return [item.strip().lower() for item in category_property.split(',')]\n",
    "            \n",
    "def get_single_item(item):\n",
    "    if not item or str(item).lower().strip() in ['nan', 'nr', '']:\n",
    "        return [None]\n",
    "    return [str(item).lower().strip()]\n",
    "\n",
    "def get_entities(title):\n",
    "    return model[title]\n",
    "\n",
    "def single_dict(raw_dict, feat, item_id):\n",
    "    if feat not in raw_dict.keys():\n",
    "        raw_dict[feat] = [item_id]\n",
    "    else:\n",
    "        current_list = raw_dict[feat]\n",
    "        current_list.append(item_id)\n",
    "        raw_dict[feat] = current_list\n",
    "\n",
    "def list_dict(raw_dict, feat_list, item_id):\n",
    "    for feat in feat_list:\n",
    "        single_dict(raw_dict, feat, item_id)\n",
    "\n",
    "def update_popularity(item_df, action_df):\n",
    "    \n",
    "        \n",
    "def sort_by_score(df):\n",
    "    logging.info(\"sort_by_score() enter, df.columns: {}\".format(df.columns))\n",
    "    df['popularity'].fillna(0, inplace=True)\n",
    "\n",
    "    df['popularity_log'] = np.log1p(df['popularity'])\n",
    "    popularity_log_max = df['popularity_log'].max()\n",
    "    popularity_log_min = df['popularity_log'].min()\n",
    "\n",
    "    df['popularity_scaled'] = ((df['popularity_log'] - popularity_log_min) / (\n",
    "            popularity_log_max - popularity_log_min)) * 10\n",
    "\n",
    "    df_sorted = df_with_score.sort_values(by='popularity_scaled', ascending=False)\n",
    "    \n",
    "    df_sorted = df_sorted.drop(\n",
    "        ['popularity_log', 'popularity_scaled'], axis=1)\n",
    "\n",
    "    logging.info(\"sort_by_score() return, df.columns: {}\".format(df_sorted.columns))\n",
    "    return df_sorted\n",
    "        \n",
    "def gen_news_id_news_property_dict(df):\n",
    "    update_popularity(df)\n",
    "    sort_by_score(df)\n",
    "    \n",
    "    news_id_news_property_dict = {}\n",
    "    news_type_news_ids_dict = {}\n",
    "    news_keywords_news_ids_dict = {}\n",
    "    news_entities_news_ids_dict = {}\n",
    "    news_words_news_ids_dict = {}\n",
    "    \n",
    "    for row in df.iterrows():\n",
    "        item_row = row[1]\n",
    "        program_id = str(item_row['news_id'])\n",
    "        current_entities = get_entities(item_row['title'])[0]\n",
    "        current_words = get_entities(item_row['title'])[1]\n",
    "        program_dict = {\n",
    "            'title': get_single_item(item_row['title']),\n",
    "            'type': get_single_item(item_row['type']),\n",
    "            'keywords': get_category(item_row['keywords']),\n",
    "            'tfidf': get_tfidf(item_row['keywords']),\n",
    "            'entities': current_entities,\n",
    "            'words': current_words\n",
    "        }\n",
    "        news_id_news_property_dict[program_id] = program_dict\n",
    "        list_dict(news_type_news_ids_dict, program_dict['type'], program_id)\n",
    "        list_dict(news_keywords_news_ids_dict, program_dict['keywords'], program_id)\n",
    "        list_dict(news_entities_news_ids_dict, program_dict['entities'], program_id)\n",
    "        list_dict(news_words_news_ids_dict, program_dict['words'], program_id)\n",
    "\n",
    "    result_dict = {\n",
    "        'news_id_news_property_dict': news_id_news_property_dict,\n",
    "        'news_type_news_ids_dict': news_type_news_ids_dict,\n",
    "        'news_keywords_news_ids_dict': news_keywords_news_ids_dict,\n",
    "        'news_entities_news_ids_dict': news_entities_news_ids_dict,\n",
    "        'news_words_news_ids_dict': news_words_news_ids_dict\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = gen_news_id_news_property_dict(df_filter_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gen_news_properties_to_news_ids_dict(df):\n",
    "    df_sorted = sort_by_score(df)\n",
    "# 1. news_id_news_property_dict.pickle\n",
    "# 2. news_type_news_ids_dict.pickle\n",
    "# 3. news_keywords_news_ids_dict.pickle\n",
    "# 4. news_entities_news_ids_dict.pickle\n",
    "# 5. news_words_news_ids_dict.pickl\n",
    "    news_type_news_ids_dict = {}\n",
    "    news_keywords_news_ids_dict = {}\n",
    "    news_entities_news_ids_dict = {}\n",
    "    news_words_news_ids_dict = {}\n",
    "\n",
    "    for row in df_sorted.iterrows():\n",
    "        item_row = row[1]\n",
    "        # program_id = {\"id\": item_row['program_id'], \"score\": item_row['cal_score'] }\n",
    "        program_id = item_row['news_id']\n",
    "        for key in [item for item in get_single_item(item_row['type']) if item is not None]:\n",
    "            news_type_news_ids_dict.setdefault(key, []).append(program_id)\n",
    "\n",
    "        for key in [item for item in get_category(item_row['keywords']) if item is not None]:\n",
    "            news_keywords_news_ids_dict.setdefault(key, []).append(program_id)\n",
    "        \n",
    "        for key in [item for item in get_category(item_row['entities']) if item is not None]:\n",
    "            news_entities_news_ids_dict.setdefault(key, []).append(program_id)\n",
    "                    \n",
    "        for key in [item for item in get_category(item_row['words']) if item is not None]:\n",
    "            news_words_news_ids_dict.setdefault(key, []).append(program_id)\n",
    "\n",
    "    result_dict = {\n",
    "        'news_type_news_ids_dict': news_type_news_ids_dict,\n",
    "        'news_keywords_news_ids_dict': news_keywords_news_ids_dict,\n",
    "        'news_entities_news_ids_dict': news_entities_news_ids_dict,\n",
    "        'news_words_news_ids_dict': news_words_news_ids_dict\n",
    "    }\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_2 = gen_news_properties_to_news_ids_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 6552418723179790856 v {'title': ['谢娜三喜临门何炅送祝福吴昕送祝福只有沈梦辰不一样'], 'type': ['news_entertainment'], 'keywords': ['杜海涛', '谢娜', '何炅', '沈梦辰', '吴昕', '快本'], 'tfidf': {'杜海涛': 1.1915032285682567, '谢娜': 0.8605173146234215, '何炅': 0.9546056150797302, '沈梦辰': 1.3327195386327908, '吴昕': 1.1915032285682567, '快本': 1.1161723746110805}, 'entities': [40191, 0, 46990, 1871, 5802, 162743, 1871, 5802, 315, 390701, 28, 302, 0, 0, 0, 0], 'words': [559632, 0, 613175, 0, 0, 754092, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552390851157295629 v {'title': ['杨幂景甜徐冬冬唐嫣不好好穿衣却美的有趣又撩人'], 'type': ['news_entertainment'], 'keywords': ['杨幂', '徐冬冬', '背带裙', '大唐荣耀', '唐嫣', '景甜'], 'tfidf': {'杨幂': 1.1161723746110805, '徐冬冬': 1.3327195386327908, '背带裙': 1.5158215867441425, '大唐荣耀': 1.5158215867441425, '唐嫣': 1.3327195386327908, '景甜': 1.3327195386327908}, 'entities': [20585, 130577, 193876, 71718, 28, 2798, 20784, 382, 727, 2, 5876, 121, 67692, 0, 0, 0], 'words': [0, 0, 359872, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552309039697494532 v {'title': ['亚洲杯夺冠赔率日本伊朗领衔中国竟与泰国并列'], 'type': ['news_sports'], 'keywords': ['土库曼斯坦', '乌兹别克斯坦', '亚洲杯', '赔率', '小组赛'], 'tfidf': {'土库曼斯坦': 1.8189859040929712, '乌兹别克斯坦': 1.8189859040929712, '亚洲杯': 1.8189859040929712, '赔率': 1.8189859040929712, '小组赛': 1.8189859040929712}, 'entities': [30465, 6489, 20485, 232, 2944, 16751, 21, 4660, 25, 2253, 13292, 0, 0, 0, 0, 0], 'words': [0, 0, 0, 0, 0, 0, 78314, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552452056043487748 v {'title': ['马夏尔要去切尔西可以商量不过穆里尼奥的要价是4000万加威廉'], 'type': ['news_sports'], 'keywords': ['威廉', '曼联', '穆里尼奥', '布莱顿', '马夏尔'], 'tfidf': {'威廉': 1.5992634463593491, '曼联': 1.339406849533297, '穆里尼奥': 1.339406849533297, '布莱顿': 1.4970983216061509, '马夏尔': 1.4970983216061509}, 'entities': [0, 64, 132, 13858, 45, 8449, 28, 230, 25044, 2, 51584, 10, 18465, 0, 0, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552466727995703815 v {'title': ['昔日中超金靴半场独造6球虐爆辽足华夏送走他后悔吗'], 'type': ['news_sports'], 'keywords': ['阿洛', '阿洛伊西奥', '华夏幸福', '埃尔纳内斯', '穆里奇'], 'tfidf': {'阿洛': 1.8189859040929712, '阿洛伊西奥': 1.5992634463593491, '华夏幸福': 1.8189859040929712, '埃尔纳内斯': 1.8189859040929712, '穆里奇': 1.5992634463593491}, 'entities': [7707, 13823, 70038, 708, 1096, 450178, 268, 2732, 0, 52005, 4709, 30724, 26, 8887, 671, 0], 'words': [0, 0, 0, 0, 0, 0, 88742, 0, 0, 0, 294528, 0, 0, 0, 0, 0]}\n",
      "k 6552404265724281357 v {'title': ['拜仁3比1逆转科隆j罗现世界级做饼'], 'type': ['news_sports'], 'keywords': ['j罗', '科隆', '拜仁', '假动作', '托利索'], 'tfidf': {'J罗': 1.8189859040929712, '科隆': 1.8189859040929712, '拜仁': 1.5992634463593491, '假动作': 1.8189859040929712, '托利索': 1.8189859040929712}, 'entities': [19108, 160, 277, 100, 8486, 34539, 0, 1507, 10631, 190, 11871, 0, 0, 0, 0, 0], 'words': [0, 0, 0, 794012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552327554336817678 v {'title': ['成都摇号选房天立世纪华府彭州清水均价5476不是投资处'], 'type': ['news_house'], 'keywords': ['都江堰', '彭州', '楼盘', '天立世纪华府', '㎡三室两厅两卫户型'], 'tfidf': {'都江堰': 1.3795409886257273, '彭州': 1.5992634463593491, '楼盘': 1.1752107391193307, '天立世纪华府': 1.5992634463593491, '㎡三室两厅两卫户型': 1.5992634463593491}, 'entities': [1564, 8527, 394, 3747, 3689, 166899, 989, 60596, 42472, 6796, 3739, 529169, 28, 10, 0, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 524554, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552236298344595982 v {'title': ['青岛这个区牛了要建奥特莱斯银座还通地铁'], 'type': ['news_house'], 'keywords': ['世茂', '银座', '高新区', '奥特莱斯', '青岛高新区', '项目投资协议'], 'tfidf': {'世茂': 1.3327195386327908, '银座': 1.5158215867441425, '高新区': 1.2475819346717922, '奥特莱斯': 1.5158215867441425, '青岛高新区': 1.5158215867441425, '项目投资协议': 1.5158215867441425}, 'entities': [2376, 43, 31, 786, 3678, 6, 64, 1715, 50349, 44926, 77, 1603, 2888, 0, 0, 0], 'words': [768682, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552134625018249735 v {'title': ['重机云集的醉美江南机车节上谁最美胡军李亚鹏告诉你答案'], 'type': ['news_car'], 'keywords': ['李亚鹏', '印第安', 'bobber', '北极星', '胡军', '车友'], 'tfidf': {'李亚鹏': 1.3327195386327908, '印第安': 1.5158215867441425, 'Bobber': 1.5158215867441425, '北极星': 1.5158215867441425, '胡军': 1.5158215867441425, '车友': 1.5158215867441425}, 'entities': [46890, 12642, 2, 0, 5125, 0, 40, 1020, 213, 727, 68081, 38631, 722, 41, 2093, 0], 'words': [0, 0, 0, 0, 227484, 0, 0, 0, 0, 0, 186541, 372574, 0, 0, 0, 0]}\n",
      "k 6552382749393551885 v {'title': ['汉腾汽车首款mpv想做车市黑马先问问宝骏730同意不'], 'type': ['news_car'], 'keywords': ['mpv', '中控台', 'gm8', 'r18', '汉腾旗下'], 'tfidf': {'MPV': 1.4970983216061509, '中控台': 1.4298038742819084, 'GM8': 1.5992634463593491, 'R18': 1.5992634463593491, '汉腾旗下': 1.8189859040929712}, 'entities': [93769, 301, 1184, 1191, 24457, 427, 190, 11956, 11745, 1008, 14235, 53589, 32647, 1493, 28, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6552296183421731336 v {'title': ['李嘉诚这次火了投资51亿新车颜值不输特斯拉董明珠都羡慕'], 'type': ['news_car'], 'keywords': ['雷克萨斯', '李嘉诚', '新车', '董明珠', '保险杠', '特斯拉'], 'tfidf': {'雷克萨斯': 1.0644798865604408, '李嘉诚': 1.025081756883069, '新车': 1.1496174905214391, '董明珠': 1.043619362734773, '保险杠': 1.3327195386327908, '特斯拉': 0.9330703264997291}, 'entities': [20703, 43, 157, 3774, 6, 104, 136653, 5827, 13458, 28, 7909, 2659, 17295, 46, 8693, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 160973, 631103, 0, 0, 0]}\n",
      "k 6552387440152150536 v {'title': ['美出台国防授权法案应对中俄竞争国防预算超7000亿美元'], 'type': ['news_military'], 'keywords': ['理查森', '北大西洋', '俄罗斯', '网络战', '司令部'], 'tfidf': {'理查森': 1.230098108259683, '北大西洋': 1.2773758638725292, '俄罗斯': 0.7727641807220538, '网络战': 1.339406849533297, '司令部': 1.1079162917950884}, 'entities': [727, 1965, 4826, 2017, 6478, 828, 33, 32, 5710, 1097, 4826, 3262, 1164, 74592, 148, 0], 'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for k, v in rd['news_id_news_property_dict'].items():\n",
    "    print(\"k {} v {}\".format(k,v))\n",
    "    if n > 10:\n",
    "        break\n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pickle_files(df_filter_action, df_filter_content, out_s3_path):\n",
    "    logging.info(f\"gen_pick_files(), \"\n",
    "                 f\"\\nbasic_s3_path={basic_s3_path}, \"\n",
    "                 f\"\\nexpend_s3_path={expend_s3_path}, \"\n",
    "                 f\"\\nout_s3_path={out_s3_path}\")\n",
    "\n",
    "    dicts_1 = gen_movie_id_movie_property_dict(df_filter_action)\n",
    "    dicts_2 = gen_movie_properties_to_movie_ids_dict(df)\n",
    "    dicts_all = dicts_1\n",
    "    dicts_all.update(dicts_2)\n",
    "    bucket, out_prefix = get_bucket_key_from_s3_path(out_s3_path)\n",
    "    for dict_name, dict_val in dicts_all.items():\n",
    "        file_name = f'{dict_name}.pickle'\n",
    "        # print(\"pickle =>\", file_name)\n",
    "        out_file = open(file_name, 'wb')\n",
    "        pickle.dump(dict_val, out_file)\n",
    "        out_file.close()\n",
    "        # s3_url = S3Uploader.upload(file_name, out_s3_path)\n",
    "        s3_url = write_to_s3(file_name, bucket, f'{out_prefix}/{file_name}')\n",
    "        logging.info(\"write {}\".format(s3_url))\n",
    "    logging.info(f\"generated {len(dicts_all)} pickle files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pickle_files(df_filter_action, df_filter_content, out_s3_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
