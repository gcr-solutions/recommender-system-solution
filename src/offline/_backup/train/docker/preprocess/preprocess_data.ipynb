{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS region:us-east-1\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import to_timestamp, to_date, lit, sort_array, collect_list, col, udf\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-002224604296/output\n"
     ]
    }
   ],
   "source": [
    "# build news map\n",
    "Timer1 = time.time()\n",
    "dataDF_complete = spark.read.format('csv').\\\n",
    "    options(header='false', inferSchema='false', encoding='utf8', escape=\"\\\"\", sep='\\001').\\\n",
    "    load('s3://sagemaker-us-east-1-002224604296/bw-data-2/')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('user_str', StringType(), True),\n",
    "    StructField('news_str', StringType(), True),\n",
    "    StructField('news_title', StringType(), True),\n",
    "    StructField('action', StringType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "]\n",
    ")\n",
    "\n",
    "df_complete = spark.createDataFrame(data = dataDF_complete.rdd, schema = schema)\n",
    "\n",
    "df_complete = df_complete.filter(df_complete.news_title != '')\n",
    "\n",
    "df_news_title_content_map = df_complete.select(\"news_str\", \"news_title\")\n",
    "df_news_title_content_map = df_news_title_content_map.distinct()\n",
    "df_news_title_content_map = df_news_title_content_map.withColumn(\"news_id\", f.row_number().over(Window.orderBy(f.monotonically_increasing_id()))-1)\n",
    "\n",
    "df_news_title_content_map = df_news_title_content_map.\\\n",
    "select(\"news_id\", \"news_str\", \"news_title\")\n",
    "print(\"build raw news id-str-title-content-type for {:.2f} minutes\".format((time.time()-Timer1)/60))\n",
    "# Save news map\n",
    "Timer1 = time.time()\n",
    "df_news_title_content_map.\\\n",
    "write.format('csv').\\\n",
    "option('header',True).mode('overwrite').\\\n",
    "option('sep','\\t').save('s3://sagemaker-us-east-1-002224604296/bw-data-com-2/news_map.csv')\n",
    "print(\"save id-str-title-content-type map for {} minutes\".format((time.time()-Timer1)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: sagemaker-recsys-graph-inference\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-recsys-graph-inference\n"
     ]
    }
   ],
   "source": [
    "# build user map\n",
    "Timer1 = time.time()\n",
    "dataDF_complete = spark.read.format('csv').\\\n",
    "    options(header='false', inferSchema='false', encoding='utf8', escape=\"\\\"\", sep='\\001').\\\n",
    "    load('s3://sagemaker-us-east-1-002224604296/bw-data-2/')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('user_str', StringType(), True),\n",
    "    StructField('news_str', StringType(), True),\n",
    "    StructField('news_title', StringType(), True),\n",
    "    StructField('action', StringType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "]\n",
    ")\n",
    "\n",
    "df_complete = spark.createDataFrame(data = dataDF_complete.rdd, schema = schema)\n",
    "\n",
    "df_user_map = df_complete.select(\"user_str\")\n",
    "df_user_map = df_user_map.distinct()\n",
    "df_user_map = df_user_map.withColumn(\"user_id\", f.row_number().over(Window.orderBy(f.monotonically_increasing_id()))-1)\n",
    "df_user_map = df_user_map.\\\n",
    "select(\"user_id\", \"user_str\")\n",
    "# print(\"build finish raw {} user id for {:.2f} minutes\".format(df_user_map.count(), (time.time()-Timer1)/60))\n",
    "\n",
    "# Save user map\n",
    "# Timer1 = time.time()\n",
    "# df_user_map.repartition(1).\\\n",
    "# write.format('csv').\\\n",
    "# option('header',True).mode('overwrite').\\\n",
    "# option('sep',',').save('file:///home/ec2-user/workplace/Aws-gcr-sc-recsys-kg/user_map.csv')\n",
    "# Timer1 = time.time()\n",
    "df_user_map.\\\n",
    "write.format('csv').\\\n",
    "option('header',True).mode('overwrite').\\\n",
    "option('sep','\\t').save('s3://sagemaker-us-east-1-002224604296/bw-data-com-2/user_map.csv')\n",
    "print(\"save user map for {} minutes\".format((time.time()-Timer1)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-recsys-graph-inference-epc-2020-11-25-8-13-27\n"
     ]
    }
   ],
   "source": [
    "# clean for second time, \n",
    "\n",
    "Timer1 = time.time()\n",
    "df_news_title_content_map = spark.read.format('csv').\\\n",
    "    options(header='true', inferSchema='false', encoding='utf8', escape=\"\\\"\",sep='\\t').\\\n",
    "    load('s3://sagemaker-us-east-1-002224604296/bw-data-com-2/news_map.csv')\n",
    "df_user_map = spark.read.format('csv').\\\n",
    "    options(header='true', inferSchema='false', encoding='utf8', escape=\"\\\"\", sep='\\t').\\\n",
    "    load('s3://sagemaker-us-east-1-002224604296/bw-data-com-2/user_map.csv')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('user_str', StringType(), True),\n",
    "    StructField('news_str', StringType(), True),\n",
    "    StructField('news_title', StringType(), True),\n",
    "    StructField('action', StringType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "]\n",
    ")\n",
    "\n",
    "dataDF = spark.read.format('csv').\\\n",
    "    options(header='false', inferSchema='false', encoding='utf8', escape=\"\\\"\", sep='\\001').\\\n",
    "    load('s3://sagemaker-us-east-1-002224604296/bw-data-2/')\n",
    "\n",
    "df = spark.createDataFrame(data=dataDF.rdd, schema=schema)\n",
    "\n",
    "# df = df.filter(df.user_str != 'userid')\n",
    "df = df.filter(df.news_title != '')\n",
    "# df_click = df.filter(df.isclick == '1')\n",
    "\n",
    "# df.show()\n",
    "\n",
    "df = df.withColumn(\"time_stamp\", to_timestamp(df.date, 'yyyy-MM-dd HH:mm:ss.SSSSSSSSS'))\n",
    "df = df.join(df_news_title_content_map.select('news_str', 'news_id'), on=['news_str'], how='left').\\\n",
    "    join(df_user_map, on=['user_str'], how='left').\\\n",
    "    select('user_id', 'news_id', 'news_title', 'action', 'time_stamp')\n",
    "\n",
    "# df.show()\n",
    "\n",
    "# last split\n",
    "filter_date = \"2020-09-27 00:00:00.000000000\"\n",
    "start_split_date = \"2020-10-04 00:00:00.000000000\"\n",
    "end_split_date = \"2020-10-16 00:00:00.000000000\"\n",
    "# end_train_val_split_date = \"2020-10-14 00:00:00.000000000\"\n",
    "# end_test_split_date = \"2020-10-16 00:00:00\"\n",
    "df = df.filter(df.time_stamp > filter_date)\n",
    "df_history = df.filter(df.time_stamp <= start_split_date)\n",
    "# df_history = df_history.filter(((df_history.action == 'recommend_article_click') | (df_history.action == 'recommend_article_read') | (df_history.action == 'recommend_article_comment') | (df_history.action == 'recommend_article_collect') | (df_history.action == 'recommend_article_share') | (df_history.action == 'recommend_article_favorite'))).select('user_id', 'news_id', 'news_title', 'time_stamp').withColumn('isclick', f.lit('1'))\n",
    "df_history = df_history.filter((df_history.action == 'recommend_article_click')).select('user_id', 'news_id', 'news_title', 'time_stamp').withColumn('isclick', f.lit('1'))\n",
    "\n",
    "\n",
    "df_data = df.filter(df.time_stamp > start_split_date)\n",
    "# df_data = df_data.withColumn('isclick', f.when(((f.col('action') == 'recommend_article_dislike') | (f.col('action') == 'recommend_article_exposure')) > 0, \"0\").\\\n",
    "#     otherwise(\"1\"))\n",
    "df_data = df_data.withColumn('isclick', f.when(f.col('action') == 'recommend_article_exposure', \"0\").\\\n",
    "    otherwise(\"1\"))\n",
    "\n",
    "filter_count = 0\n",
    "df_history_click_sum = df_history.groupBy('user_id').agg(\n",
    "    {'isclick': 'sum'}).withColumnRenamed('sum(isclick)', \"sum_click\")\n",
    "\n",
    "df_history_click_seed = df_history_click_sum.filter(\n",
    "    df_history_click_sum.sum_click > filter_count)\n",
    "\n",
    "df_click_train = df_history.join(df_history_click_seed, df_history.user_id == df_history_click_seed.user_id, \"inner\").\\\n",
    "    select(df_history.user_id, df_history.news_id,\n",
    "           df_history.isclick, df_history.time_stamp)\n",
    "\n",
    "# df_click_train.show()\n",
    "\n",
    "train_dataDF = spark.read.format('csv').\\\n",
    "    options(header='false', inferSchema='false', encoding='utf8', escape=\"\\\"\", sep=',').\\\n",
    "    load('s3://sagemaker-us-east-1-002224604296/bw-data-com-2/news_encoding.csv')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('news_id', StringType(), True),\n",
    "    StructField('news_words', StringType(), True),\n",
    "    StructField('news_entities', StringType(), True),\n",
    "]\n",
    ")\n",
    "\n",
    "df_news_word_entity = spark.createDataFrame(\n",
    "    data=train_dataDF.rdd, schema=schema)\n",
    "\n",
    "df_click_train = df_click_train.join(\n",
    "    df_news_word_entity, on=['news_id'], how='left')\n",
    "\n",
    "# df_news_word_entity.filter(col('news_id')==37503).show()\n",
    "\n",
    "# df_click_train.filter(col('user_id')==837256).show()\n",
    "\n",
    "window = Window.partitionBy(\"user_id\").orderBy(f.desc(\"time_stamp\"))\n",
    "\n",
    "df_click = df_click_train \\\n",
    "    .withColumn(\"clicked_words_list\", f.collect_list(\"news_words\").over(window))\\\n",
    "    .withColumn(\"clicked_entities_list\", f.collect_list(\"news_entities\").over(window))\\\n",
    "    .select(\"user_id\", \"news_words\", \"news_entities\", \"isclick\", \"clicked_words_list\", \"clicked_entities_list\")\n",
    "\n",
    "# df_click.filter(col('user_id')==837256).show()\n",
    "# df_click.show()\n",
    "\n",
    "df_click = df_click.groupby(df_click.user_id).agg(f.element_at(f.collect_list(\"clicked_words_list\"), -1).alias(\"clicked_words\"),\n",
    "                                                  f.element_at(f.collect_list(\"clicked_entities_list\"), -1).alias(\"clicked_entities\"))\n",
    "\n",
    "df_click = df_user_map.join(df_click, on=\"user_id\", how=\"left\")\n",
    "\n",
    "df_click_history = df_click.select(\"user_id\", f.concat_ws(\"-\", f.col(\"clicked_words\").cast(\"array<string>\")).alias(\n",
    "    \"clicked_words\"), f.concat_ws(\"-\", f.col(\"clicked_entities\").cast(\"array<string>\")).alias(\"clicked_entities\"))\n",
    "\n",
    "# construct\n",
    "df_data_with_history = df_data.alias(\"a\").join(df_history_click_seed.alias('b'), df_data.user_id == df_history_click_seed.user_id, 'inner').\\\n",
    "    select('a.user_id', 'a.news_id', 'a.isclick', 'a.time_stamp')\n",
    "\n",
    "df_user = df_data_with_history.select(\n",
    "    'user_id', 'news_id', 'isclick', 'time_stamp')\n",
    "\n",
    "df_user_with_word_entity = df_user.join(df_news_word_entity, on=['news_id'], how='left'). \\\n",
    "    select(\"user_id\", \"news_words\", \"news_entities\",\n",
    "           \"isclick\", \"news_id\", \"time_stamp\")\n",
    "\n",
    "df_complete = df_user_with_word_entity.join(df_click_history, on=['user_id'], how='left'). \\\n",
    "    select(\"user_id\", \"news_words\", \"news_entities\", \"isclick\",\n",
    "           \"clicked_words\", \"clicked_entities\", \"news_id\", 'time_stamp')\n",
    "\n",
    "df_train, df_test = df_complete.randomSplit([0.9, 0.1], 13)\n",
    "\n",
    "\n",
    "# df_train.show()\n",
    "\n",
    "# df_test.show()\n",
    "\n",
    "df_train.write.format('csv').\\\n",
    "    option('header', False).mode('overwrite').option('sep', '\\t').\\\n",
    "    save('s3://sagemaker-us-east-1-002224604296/bw-data-clean-2-complete-update-train-2-1.csv')\n",
    "\n",
    "df_test.write.format('csv').\\\n",
    "    option('header', False).mode('overwrite').option('sep', '\\t').\\\n",
    "    save('s3://sagemaker-us-east-1-002224604296/bw-data-clean-2-complete-update-test-2-1.csv')\n",
    "\n",
    "print(\"It take {:.2f} mimutes to finish\".format((time.time()-Timer1)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}